{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4697938b-42f2-4c05-b124-03b8fcbc2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "from nltk.lm import Vocabulary\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54bd570-6c1e-4c1f-9129-a4609d542eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/souvik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/souvik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/souvik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206e5e6c-5ce5-4399-bdd3-eff0c1cdc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Harry_Potter_all_books_preprocessed.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "470fa517-2ce0-4119-9f3c-7b893e644cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce139664-e102-4a69-be40-7514044c9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "tokens = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47eaf41f-d42f-4f90-b6ca-ec886912b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boy', 'lived', 'mr', 'mr', 'dursley', 'number', 'four', 'privet', 'drive', 'proud', 'say', 'perfectly', 'normal', 'thank', 'much', '.they', 'last', 'people', 'youd', 'expect', 'involved', 'anything', 'strange', 'mysterious', 'didnt', 'hold', 'nonsense', '.mr', 'dursley', 'director', 'firm', 'called', 'grunnings', 'made', 'drill', '.he', 'big', 'beefy', 'man', 'hardly', 'neck', 'although', 'large', 'mustache', '.mrs', 'dursley', 'thin', 'blonde', 'nearly', 'twice']\n"
     ]
    }
   ],
   "source": [
    "# Use only the first 10,000 words\n",
    "tokens = tokens[:10000]\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57aa0b5e-20de-48a4-8e80-b90a866612e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = Vocabulary(tokens)\n",
    "# Prepare data for training\n",
    "train_data, padded_sents = padded_everygram_pipeline(2, tokens)\n",
    "\n",
    "# Fit MLE bigram model\n",
    "mle_bigram_model = MLE(2)\n",
    "mle_bigram_model.fit(train_data, vocab)\n",
    "\n",
    "# Fit Kneser-Ney bigram model\n",
    "kn_model = KneserNeyInterpolated(2)\n",
    "kn_model.fit(train_data, vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89571eb4-758b-47f6-ac6e-f28fc98bc1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text using MLE bigram model starting with 'Harry Potter':\n",
      "Harry Potter u > > > > > > > > > > > > > > > > >\n",
      "\n",
      "Generated text using Kneser-Ney bigram model starting with 'Harry Potter':\n",
      "Text generation failed. Insufficient data for given starting words.\n",
      "\n",
      "Generated text using MLE bigram model starting with 'Dumbledore':\n",
      "Dumbledore > > > > > > > > > > k > > > > > > > >\n",
      "\n",
      "Generated text using Kneser-Ney bigram model starting with 'Dumbledore':\n",
      "Text generation failed. Insufficient data for given starting words.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate text using a language model\n",
    "def generate_text(model, start_words, max_length=20):\n",
    "    generated_text = start_words.split()\n",
    "    try:\n",
    "        while len(generated_text) < max_length:\n",
    "            next_word = model.generate(1, text_seed=generated_text)[-1]\n",
    "            generated_text.append(next_word)\n",
    "            if next_word == '<s>':  # If the end of the sentence is reached\n",
    "                break\n",
    "    except ValueError:\n",
    "        return \"Text generation failed. Insufficient data for given starting words.\"\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Generate text using MLE bigram model starting with \"Harry Potter\"\n",
    "generated_text_mle_hp = generate_text(mle_bigram_model, \"Harry Potter\")\n",
    "print(\"Generated text using MLE bigram model starting with 'Harry Potter':\")\n",
    "print(generated_text_mle_hp)\n",
    "\n",
    "# Generate text using Kneser-Ney bigram model starting with \"Harry Potter\"\n",
    "generated_text_kn_hp = generate_text(kn_model, \"Harry Potter\")\n",
    "print(\"\\nGenerated text using Kneser-Ney bigram model starting with 'Harry Potter':\")\n",
    "print(generated_text_kn_hp)\n",
    "\n",
    "# Generate text using MLE bigram model starting with \"Dumbledore\"\n",
    "generated_text_mle_dumbledore = generate_text(mle_bigram_model, \"Dumbledore\")\n",
    "print(\"\\nGenerated text using MLE bigram model starting with 'Dumbledore':\")\n",
    "print(generated_text_mle_dumbledore)\n",
    "\n",
    "# Generate text using Kneser-Ney bigram model starting with \"Dumbledore\"\n",
    "generated_text_kn_dumbledore = generate_text(kn_model, \"Dumbledore\")\n",
    "print(\"\\nGenerated text using Kneser-Ney bigram model starting with 'Dumbledore':\")\n",
    "print(generated_text_kn_dumbledore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a849b99-5963-43ff-8c79-10482f121ecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Preprocess and tokenize the dataset using NLTK\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming 'text' is your dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     13\u001b[0m filtered_text \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Fit two bigram language models on the text: MLE and Kneser-Ney discounting\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import MLE, KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# 1. Preprocess and tokenize the dataset using NLTK\n",
    "# Assuming 'text' is your dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text.lower())\n",
    "filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# 2. Fit two bigram language models on the text: MLE and Kneser-Ney discounting\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, filtered_text)\n",
    "\n",
    "# MLE Model\n",
    "mle_model = MLE(n)\n",
    "mle_model.fit(train_data, padded_sents)\n",
    "\n",
    "# Kneser-Ney Model\n",
    "kn_model = KneserNeyInterpolated(n)\n",
    "kn_model.fit(train_data, padded_sents)\n",
    "\n",
    "# 3. Generate text using both the language models\n",
    "mle_text = ' '.join(mle_model.generate(20, text_seed=['Harry', 'Potter']))\n",
    "kn_text = ' '.join(kn_model.generate(20, text_seed=['Dumbledore']))\n",
    "\n",
    "print(\"MLE generated text: \", mle_text)\n",
    "print(\"Kneser-Ney generated text: \", kn_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade57dfc-ce94-4919-8cdc-2d56a8023a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
